{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yJK0ECjE5vK"
   },
   "source": [
    "### This Notebook is used to understand Aprioiri Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivfAbVvYHrRA"
   },
   "source": [
    "**Following Cell is used to load a test dataset containing 5 records. But You Should use your funcion to load the datset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Rank                                                      1\n",
      "School                           California Institute of Technology\n",
      "State                                                            CA\n",
      "Undergrad. Enrollment                                           939\n",
      "Admission Rate                                                  21%\n",
      "*SAT or ACT                                                 99/100%\n",
      "Student/faculty Ratio                                             3\n",
      "4-year Grad. Rate                                               71%\n",
      "6-year Grad. Rate                                               85%\n",
      "Quality Rank                                                     10\n",
      "Total Costs                                                $32,682 \n",
      "Cost After Need-based Aid                                  $10,981 \n",
      "Need Met                                                       100%\n",
      "Aid From Grants                                                 93%\n",
      "Cost After Non-Need-Based Aid                              $18,553 \n",
      "Non-Need-Based Aid+                                             15%\n",
      "Average Debt                                               $10,244 \n",
      "Cost Rank                                                         4\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Load College Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "college = pd.read_csv(\"Top 100 Private Colleges.2003.csv\")\n",
    "print(college.loc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after cleaning : 100\n",
      "{'CT', 'NH', 'MN', 'IL', 'MO', 'TN', 'OR', 'TX', 'KY', 'MD', 'ME', 'IN', 'GA', 'MA', 'RI', 'VT', 'WI', 'MI', 'SC', 'UT', 'NY', 'CA', 'VA', 'NJ', 'PA', 'NC', 'CO', 'OH', 'IA', 'DC', 'WA'}\n"
     ]
    }
   ],
   "source": [
    "#removing columns having nan values\n",
    "column_list = [\"Overall Rank\",\"Quality Rank\",\"Cost Rank\"]\n",
    "\n",
    "for column_i in column_list:\n",
    "    college = college.drop(college.index[college[column_i].isin([np.nan])])\n",
    "print(\"Data after cleaning :\",len(college))\n",
    "\n",
    "\n",
    "list_state = set(list(college[\"State\"]))\n",
    "print(list_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToClasses(x):\n",
    "    if x<34:\n",
    "        return column_name + \" Good\"\n",
    "    elif x<67:\n",
    "        return column_name + \" Average\"\n",
    "    else:\n",
    "        return column_name + \" Bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Rank                                      Overall Rank Good\n",
      "School                           California Institute of Technology\n",
      "State                                                            CA\n",
      "Undergrad. Enrollment                                           939\n",
      "Admission Rate                                                  21%\n",
      "*SAT or ACT                                                 99/100%\n",
      "Student/faculty Ratio                                             3\n",
      "4-year Grad. Rate                                               71%\n",
      "6-year Grad. Rate                                               85%\n",
      "Quality Rank                                      Quality Rank Good\n",
      "Total Costs                                                $32,682 \n",
      "Cost After Need-based Aid                                  $10,981 \n",
      "Need Met                                                       100%\n",
      "Aid From Grants                                                 93%\n",
      "Cost After Non-Need-Based Aid                              $18,553 \n",
      "Non-Need-Based Aid+                                             15%\n",
      "Average Debt                                               $10,244 \n",
      "Cost Rank                                            Cost Rank Good\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "column_name = ''\n",
    "for column_name in column_list:\n",
    "    college[column_name] = college[column_name].apply(ToClasses)\n",
    "\n",
    "print(college.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State                          NY\n",
       "Quality Rank     Quality Rank Bad\n",
       "Overall Rank     Overall Rank Bad\n",
       "Cost Rank       Cost Rank Average\n",
       "Name: 89, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_wise_data = college.sort_values(by='State').loc[:,[ 'State','Quality Rank','Overall Rank','Cost Rank']]\n",
    "state_wise_data.loc[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "j=0\n",
    "ranks = ['Cost Rank Average', 'Cost Rank Bad', 'Cost Rank Good', \n",
    "         'Overall Rank Average', 'Overall Rank Bad', 'Overall Rank Good', \n",
    "         'Quality Rank Average', 'Quality Rank Bad', 'Quality Rank Good']\n",
    "for rank in ranks:\n",
    "    val = set()\n",
    "    for index, item in state_wise_data.iterrows():\n",
    "        if item['Cost Rank'] == rank:\n",
    "            val.update([item['State']])\n",
    "        if item['Quality Rank'] == rank:\n",
    "            val.update([item['State']])\n",
    "        if item['Overall Rank'] == rank:\n",
    "            val.update([item['State']])\n",
    "    data.append(list(val))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['CT', 'WI', 'CA', 'NJ', 'MN', 'ME', 'NC', 'GA', 'OH', 'PA', 'OR', 'NY', 'MA', 'MD', 'IN', 'WA'], ['CT', 'VT', 'CA', 'NH', 'NC', 'PA', 'IL', 'MO', 'TN', 'NY', 'DC', 'MA', 'MD', 'ME', 'RI'], ['MN', 'IL', 'MO', 'TN', 'TX', 'KY', 'IN', 'MA', 'WI', 'MI', 'SC', 'UT', 'NY', 'CA', 'VA', 'PA', 'CO', 'OH', 'IA'], ['CT', 'MN', 'IL', 'TN', 'KY', 'ME', 'IN', 'MA', 'RI', 'VT', 'WI', 'SC', 'UT', 'NY', 'CA', 'NC', 'PA', 'OH', 'IA'], ['CT', 'MN', 'IL', 'MO', 'TN', 'OR', 'TX', 'MD', 'IN', 'MA', 'RI', 'WI', 'MI', 'NY', 'CA', 'PA', 'OH', 'DC', 'WA'], ['CT', 'CA', 'NH', 'NC', 'ME', 'CO', 'GA', 'IL', 'MO', 'NJ', 'PA', 'VA', 'NY', 'TX', 'DC', 'MA', 'MD', 'IN'], ['CT', 'CA', 'VA', 'MN', 'NC', 'CO', 'OH', 'IL', 'PA', 'TN', 'IA', 'NY', 'MA', 'IN'], ['MN', 'IL', 'MO', 'OR', 'TX', 'KY', 'MD', 'IN', 'MA', 'RI', 'WI', 'MI', 'SC', 'UT', 'NY', 'CA', 'PA', 'OH', 'DC', 'WA'], ['CT', 'VT', 'CA', 'NH', 'NC', 'NJ', 'PA', 'GA', 'IL', 'MO', 'VA', 'NY', 'TX', 'DC', 'MA', 'MD', 'ME', 'RI']]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "7AMFPyo964Z8",
    "outputId": "097cea36-bc42-4942-a05d-feae671d5f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[['CT',\n",
      "  'WI',\n",
      "  'CA',\n",
      "  'NJ',\n",
      "  'MN',\n",
      "  'ME',\n",
      "  'NC',\n",
      "  'GA',\n",
      "  'OH',\n",
      "  'PA',\n",
      "  'OR',\n",
      "  'NY',\n",
      "  'MA',\n",
      "  'MD',\n",
      "  'IN',\n",
      "  'WA'],\n",
      " ['CT',\n",
      "  'VT',\n",
      "  'CA',\n",
      "  'NH',\n",
      "  'NC',\n",
      "  'PA',\n",
      "  'IL',\n",
      "  'MO',\n",
      "  'TN',\n",
      "  'NY',\n",
      "  'DC',\n",
      "  'MA',\n",
      "  'MD',\n",
      "  'ME',\n",
      "  'RI'],\n",
      " ['MN',\n",
      "  'IL',\n",
      "  'MO',\n",
      "  'TN',\n",
      "  'TX',\n",
      "  'KY',\n",
      "  'IN',\n",
      "  'MA',\n",
      "  'WI',\n",
      "  'MI',\n",
      "  'SC',\n",
      "  'UT',\n",
      "  'NY',\n",
      "  'CA',\n",
      "  'VA',\n",
      "  'PA',\n",
      "  'CO',\n",
      "  'OH',\n",
      "  'IA'],\n",
      " ['CT',\n",
      "  'MN',\n",
      "  'IL',\n",
      "  'TN',\n",
      "  'KY',\n",
      "  'ME',\n",
      "  'IN',\n",
      "  'MA',\n",
      "  'RI',\n",
      "  'VT',\n",
      "  'WI',\n",
      "  'SC',\n",
      "  'UT',\n",
      "  'NY',\n",
      "  'CA',\n",
      "  'NC',\n",
      "  'PA',\n",
      "  'OH',\n",
      "  'IA'],\n",
      " ['CT',\n",
      "  'MN',\n",
      "  'IL',\n",
      "  'MO',\n",
      "  'TN',\n",
      "  'OR',\n",
      "  'TX',\n",
      "  'MD',\n",
      "  'IN',\n",
      "  'MA',\n",
      "  'RI',\n",
      "  'WI',\n",
      "  'MI',\n",
      "  'NY',\n",
      "  'CA',\n",
      "  'PA',\n",
      "  'OH',\n",
      "  'DC',\n",
      "  'WA'],\n",
      " ['CT',\n",
      "  'CA',\n",
      "  'NH',\n",
      "  'NC',\n",
      "  'ME',\n",
      "  'CO',\n",
      "  'GA',\n",
      "  'IL',\n",
      "  'MO',\n",
      "  'NJ',\n",
      "  'PA',\n",
      "  'VA',\n",
      "  'NY',\n",
      "  'TX',\n",
      "  'DC',\n",
      "  'MA',\n",
      "  'MD',\n",
      "  'IN'],\n",
      " ['CT',\n",
      "  'CA',\n",
      "  'VA',\n",
      "  'MN',\n",
      "  'NC',\n",
      "  'CO',\n",
      "  'OH',\n",
      "  'IL',\n",
      "  'PA',\n",
      "  'TN',\n",
      "  'IA',\n",
      "  'NY',\n",
      "  'MA',\n",
      "  'IN'],\n",
      " ['MN',\n",
      "  'IL',\n",
      "  'MO',\n",
      "  'OR',\n",
      "  'TX',\n",
      "  'KY',\n",
      "  'MD',\n",
      "  'IN',\n",
      "  'MA',\n",
      "  'RI',\n",
      "  'WI',\n",
      "  'MI',\n",
      "  'SC',\n",
      "  'UT',\n",
      "  'NY',\n",
      "  'CA',\n",
      "  'PA',\n",
      "  'OH',\n",
      "  'DC',\n",
      "  'WA'],\n",
      " ['CT',\n",
      "  'VT',\n",
      "  'CA',\n",
      "  'NH',\n",
      "  'NC',\n",
      "  'NJ',\n",
      "  'PA',\n",
      "  'GA',\n",
      "  'IL',\n",
      "  'MO',\n",
      "  'VA',\n",
      "  'NY',\n",
      "  'TX',\n",
      "  'DC',\n",
      "  'MA',\n",
      "  'MD',\n",
      "  'ME',\n",
      "  'RI']]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Loads an example of market basket transactions for testing purposes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list (database) of lists (transactions). Each element of a transaction \n",
    "    is an item.\n",
    "    \n",
    "    return [['Bread', 'Milk'], \n",
    "            ['Bread', 'Diapers', 'Beer', 'Eggs'], \n",
    "            ['Milk', 'Diapers', 'Beer', 'Coke'], \n",
    "            ['Bread', 'Milk', 'Diapers', 'Beer'], \n",
    "            ['Bread', 'Milk', 'Diapers', 'Coke']]\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "dataset = load_dataset() # list of transactions; each transaction is a list of items\n",
    "D = map(set, dataset) # set of transactions; each transaction is a list of items\n",
    "print(len(dataset))\n",
    "pprint.pprint(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3-Qms981jt9"
   },
   "outputs": [],
   "source": [
    "def create_candidates(dataset, VERBOSE=False):\n",
    "    \"\"\"Creates a list of candidate 1-itemsets from a list of transactions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate \n",
    "        itemsets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The list of candidate itemsets (c1) passed as a frozenset (a set that is \n",
    "    immutable and hashable).\n",
    "    \"\"\"\n",
    "    c1 = [] # list of all items in the database of transactions\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if not [item] in c1:\n",
    "                c1.append([item])\n",
    "    c1.sort()\n",
    "\n",
    "    if VERBOSE:\n",
    "        # Print a list of all the candidate items.\n",
    "        print (\"\" \\\n",
    "            + \"{\" \\\n",
    "            + \"\".join(str(i[0]) + \", \" for i in iter(c1)).rstrip(', ') \\\n",
    "            + \"}\")\n",
    "\n",
    "    # Map c1 to a frozenset because it will be the key of a dictionary.\n",
    "    return map(frozenset, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FpFZo6K778OX",
    "outputId": "ff612e9b-a318-480c-9f29-e3b4b594ea64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{CA, CO, CT, DC, GA, IA, IL, IN, KY, MA, MD, ME, MI, MN, MO, NC, NH, NJ, NY, OH, OR, PA, RI, SC, TN, TX, UT, VA, VT, WA, WI}\n"
     ]
    }
   ],
   "source": [
    "# Generate candidate itemsets.\n",
    "C1 = create_candidates(dataset, VERBOSE=True) # candidate 1-itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNCB3kH9J4VP"
   },
   "source": [
    "**Now a function is defined to prune the candidates from the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92M3wSFTJ_pm"
   },
   "outputs": [],
   "source": [
    "def support_prune(dataset, candidates, min_support, VERBOSE=False):\n",
    "    \"\"\"Returns all candidate itemsets that meet a minimum support threshold.\n",
    "\n",
    "    By the apriori principle, if an itemset is frequent, then all of its \n",
    "    subsets must also be frequent. As a result, we can perform support-based \n",
    "    pruning to systematically control the exponential growth of candidate \n",
    "    itemsets. Thus, itemsets that do not meet the minimum support level are \n",
    "    pruned from the input list of itemsets (dataset).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate \n",
    "        itemsets.\n",
    "\n",
    "    candidates : frozenset\n",
    "        The list of candidate itemsets.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    retlist : list\n",
    "        The list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "    \"\"\"\n",
    "    \n",
    "    sscnt = {} # set for support counts\n",
    "    can = []\n",
    "    for i in candidates:\n",
    "        can.append(list(i))\n",
    "    dta = map(list, dataset)\n",
    "    for i in can:\n",
    "        sscnt[frozenset(i)] = 0\n",
    "    num_items=0\n",
    "    for tid in dta:\n",
    "        num_items+=1\n",
    "        for candidate in can:\n",
    "            if candidate[0] in tid:\n",
    "                sscnt[frozenset(candidate)] += 1\n",
    "\n",
    "    #num_items = 5 #len(dataset) # total number of transactions in the dataset\n",
    "    retlist = [] # array for unpruned itemsets\n",
    "    support_data = {} # set for support data for corresponding itemsets\n",
    "    for key in sscnt:\n",
    "        # Calculate the support of itemset key.\n",
    "        support = sscnt[key] / num_items\n",
    "        if support >= min_support:\n",
    "            retlist.insert(0, key)\n",
    "        support_data[key] = support\n",
    "\n",
    "    # Print a list of the pruned itemsets.\n",
    "    if VERBOSE:\n",
    "        for kset in retlist:\n",
    "            for item in kset:\n",
    "                print (\"{\" + str(item) + \"}\")\n",
    "        #print\n",
    "        for key in sscnt:\n",
    "            print (\"\" \\\n",
    "                + \"{\" \\\n",
    "                + \"\".join([str(i) + \", \" for i in iter(key)]).rstrip(', ') \\\n",
    "                + \"}\" \\\n",
    "                + \":  sup = \" + str(support_data[key]))\n",
    "\n",
    "    return retlist, support_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yX97iKJQK9TA"
   },
   "source": [
    "**Now Test this Function Support_prune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TIi-Pa-18DVw",
    "outputId": "9d1dbb4d-b54f-4eef-a7a6-d3f21e66c5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{PA}\n",
      "{NY}\n",
      "{MA}\n",
      "{IL}\n",
      "{CA}\n",
      "{CA}:  sup = 1.0\n",
      "{CO}:  sup = 0.3333333333333333\n",
      "{CT}:  sup = 0.7777777777777778\n",
      "{DC}:  sup = 0.5555555555555556\n",
      "{GA}:  sup = 0.3333333333333333\n",
      "{IA}:  sup = 0.3333333333333333\n",
      "{IL}:  sup = 0.8888888888888888\n",
      "{IN}:  sup = 0.7777777777777778\n",
      "{KY}:  sup = 0.3333333333333333\n",
      "{MA}:  sup = 1.0\n",
      "{MD}:  sup = 0.6666666666666666\n",
      "{ME}:  sup = 0.5555555555555556\n",
      "{MI}:  sup = 0.3333333333333333\n",
      "{MN}:  sup = 0.6666666666666666\n",
      "{MO}:  sup = 0.6666666666666666\n",
      "{NC}:  sup = 0.6666666666666666\n",
      "{NH}:  sup = 0.3333333333333333\n",
      "{NJ}:  sup = 0.3333333333333333\n",
      "{NY}:  sup = 1.0\n",
      "{OH}:  sup = 0.6666666666666666\n",
      "{OR}:  sup = 0.3333333333333333\n",
      "{PA}:  sup = 1.0\n",
      "{RI}:  sup = 0.5555555555555556\n",
      "{SC}:  sup = 0.3333333333333333\n",
      "{TN}:  sup = 0.5555555555555556\n",
      "{TX}:  sup = 0.5555555555555556\n",
      "{UT}:  sup = 0.3333333333333333\n",
      "{VA}:  sup = 0.4444444444444444\n",
      "{VT}:  sup = 0.3333333333333333\n",
      "{WA}:  sup = 0.3333333333333333\n",
      "{WI}:  sup = 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets.\n",
    "F1, support_data = support_prune(D, C1, 0.8, VERBOSE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VsdhM5LPyyw"
   },
   "source": [
    "**Now We will Make two functions 'Apriori' and 'Apriori Gen'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAAUmI2KBYs"
   },
   "outputs": [],
   "source": [
    "def apriori(dataset, min_support=0.5, VERBOSE=False):\n",
    "    \"\"\"Implements the Apriori algorithm.\n",
    "\n",
    "    The Apriori algorithm will iteratively generate new candidate \n",
    "    k-itemsets using the frequent (k-1)-itemsets found in the previous \n",
    "    iteration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate itemsets.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    F : list\n",
    "        The list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    C1 = create_candidates(dataset)\n",
    "    D = map(set, dataset)\n",
    "    F1, support_data = support_prune(D, C1, min_support, VERBOSE=False) # prune candidate 1-itemsets\n",
    "    F = [F1] # list of frequent itemsets; initialized to frequent 1-itemsets\n",
    "    k = 2 # the itemset cardinality\n",
    "    while (len(F[k - 2]) > 0):\n",
    "        Ck = apriori_gen(F[k-2], k) # generate candidate itemsets\n",
    "        Fk, supK = support_prune(D, Ck, min_support) # prune candidate itemsets\n",
    "        support_data.update(supK) # update the support counts to reflect pruning\n",
    "        F.append(Fk) # add the pruned candidate itemsets to the list of frequent itemsets\n",
    "        k += 1\n",
    "\n",
    "    if VERBOSE:\n",
    "        # Print a list of all the frequent itemsets.\n",
    "        for kset in F:\n",
    "            for item in kset:\n",
    "                print( \"\" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join(str(i) + \", \" for i in iter(item)).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \":  sup = \" + str(round(support_data[item], 3)))\n",
    "\n",
    "    return F, support_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8X3orG5QtX1"
   },
   "outputs": [],
   "source": [
    "def apriori_gen(freq_sets, k):\n",
    "    \"\"\"Generates candidate itemsets (via the F_k-1 x F_k-1 method).\n",
    "\n",
    "    This operation generates new candidate k-itemsets based on the frequent \n",
    "    (k-1)-itemsets found in the previous iteration. The candidate generation \n",
    "    procedure merges a pair of frequent (k-1)-itemsets only if their first k-2 \n",
    "    items are identical.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_sets : list\n",
    "        The list of frequent (k-1)-itemsets.\n",
    "\n",
    "    k : integer\n",
    "        The cardinality of the current itemsets begin evaluated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    retlist : list\n",
    "        The list of merged frequent itemsets.\n",
    "    \"\"\"\n",
    "    retList = [] # list of merged frequent itemsets\n",
    "    lenLk = len(freq_sets) # number of frequent itemsets\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i+1, lenLk):\n",
    "            a=list(freq_sets[i])\n",
    "            b=list(freq_sets[j])\n",
    "            a.sort()\n",
    "            b.sort()\n",
    "            F1 = a[:k-2] # first k-2 items of freq_sets[i]\n",
    "            F2 = b[:k-2] # first k-2 items of freq_sets[j]\n",
    "\n",
    "            if F1 == F2: # if the first k-2 items are identical\n",
    "                # Merge the frequent itemsets.\n",
    "                retList.append(freq_sets[i] | freq_sets[j])\n",
    "\n",
    "    return retList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5uFcJkTTjSr"
   },
   "source": [
    "#### Now Test Run the Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "POdWUhDMTOqZ",
    "outputId": "35d10cba-6ea1-4c49-fc38-83b1638f4b9a"
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-69a9f3c9406c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate all the frequent itemsets using the Apriori algorithm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVERBOSE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-37e8cce84131>\u001b[0m in \u001b[0;36mapriori\u001b[0;34m(dataset, min_support, VERBOSE)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mCk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapriori_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# generate candidate itemsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mFk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport_prune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# prune candidate itemsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0msupport_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update the support counts to reflect pruning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add the pruned candidate itemsets to the list of frequent itemsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-500bb666c5ff>\u001b[0m in \u001b[0;36msupport_prune\u001b[0;34m(dataset, candidates, min_support, VERBOSE)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msscnt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Calculate the support of itemset key.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msscnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mretlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Generate all the frequent itemsets using the Apriori algorithm.\n",
    "F, support_data = apriori(dataset, min_support=0.02, VERBOSE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZCRcIqCVY-S"
   },
   "source": [
    "Now Functions to generate Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILTy7Kl4Twnj"
   },
   "outputs": [],
   "source": [
    "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.7):\n",
    "    \"\"\"Generates a set of candidate rules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_set : frozenset\n",
    "        The complete list of frequent itemsets.\n",
    "\n",
    "    H : list\n",
    "        A list of frequent itemsets (of a particular length).\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "\n",
    "    rules : list\n",
    "        A potentially incomplete set of candidate rules above the minimum \n",
    "        confidence threshold.\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.7.\n",
    "    \"\"\"\n",
    "    m = len(H[0])\n",
    "    if (len(freq_set) > (m+1)):\n",
    "        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n",
    "        Hmp1 = calc_confidence(freq_set, Hmp1,  support_data, rules, min_confidence)\n",
    "        if len(Hmp1) > 1:\n",
    "            # If there are candidate rules above the minimum confidence \n",
    "            # threshold, recurse on the list of these candidate rules.\n",
    "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence)\n",
    "\n",
    "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.7, VERBOSE=False):\n",
    "    \"\"\"Evaluates the generated rules.\n",
    "\n",
    "    One measurement for quantifying the goodness of association rules is \n",
    "    confidence. The confidence for a rule 'P implies H' (P -> H) is defined as \n",
    "    the support for P and H divided by the support for P \n",
    "    (support (P|H) / support(P)), where the | symbol denotes the set union \n",
    "    (thus P|H means all the items in set P or in set H).\n",
    "\n",
    "    To calculate the confidence, we iterate through the frequent itemsets and \n",
    "    associated support data. For each frequent itemset, we divide the support \n",
    "    of the itemset by the support of the antecedent (left-hand-side of the \n",
    "    rule).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_set : frozenset\n",
    "        The complete list of frequent itemsets.\n",
    "\n",
    "    H : list\n",
    "        A frequent itemset.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold.\n",
    "\n",
    "    rules : list\n",
    "        A potentially incomplete set of candidate rules above the minimum \n",
    "        confidence threshold.\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.7.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pruned_H : list\n",
    "        The list of candidate rules above the minimum confidence threshold.\n",
    "    \"\"\"\n",
    "    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n",
    "    for conseq in H: # iterate over the frequent itemsets\n",
    "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
    "        if conf >= min_confidence:\n",
    "\n",
    "            rules.append((freq_set - conseq, conseq, conf))\n",
    "            pruned_H.append(conseq)\n",
    "\n",
    "            if VERBOSE:\n",
    "                print( \"\" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \" ---> \" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \":  conf = \" + str(round(conf, 3)) \\\n",
    "                    + \", sup = \" + str(round(support_data[freq_set], 3)))\n",
    "\n",
    "    return pruned_H\n",
    "\n",
    "def generate_rules(F, support_data, min_confidence, VERBOSE=False):\n",
    "    \"\"\"Generates a set of candidate rules from a list of frequent itemsets.\n",
    "\n",
    "    For each frequent itemset, we calculate the confidence of using a\n",
    "    particular item as the rule consequent (right-hand-side of the rule). By \n",
    "    testing and merging the remaining rules, we recursively create a list of \n",
    "    pruned rules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    F : list\n",
    "        A list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The corresponding support data for the frequent itemsets (L).\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.7.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rules : list\n",
    "        The list of candidate rules above the minimum confidence threshold.\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    for i in range(1, len(F)):\n",
    "        for freq_set in F[i]:\n",
    "            H1 = [frozenset([item]) for item in freq_set]\n",
    "\n",
    "            if (i > 1):\n",
    "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence)\n",
    "            else:\n",
    "                calc_confidence(freq_set, H1, support_data, rules, min_confidence, VERBOSE=VERBOSE)\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_JMaZMCVfhW"
   },
   "source": [
    "Now test the Genration of Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSQNfAJHVejn"
   },
   "outputs": [],
   "source": [
    "# Generate the association rules from a list of frequent itemsets.\n",
    "H = generate_rules(F, support_data, min_confidence=0.01, VERBOSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
